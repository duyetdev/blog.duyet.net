---
template: post
title: Running Apache Spark on Kubernetes at Fossil
date: '2021-02-20T00:00:00.000+07:00'
author: Van-Duyet Le
category: Data Engineer
tags:
  - Data Engineer
  - Apache Spark
  - Big Data
thumbnail:
slug: /2021/02/running-apache-spark-on-kubernetes-at-fossil.html
draft: true
description: Running Apache at Fossil
fbCommentUrl: none
---

At Fossil, we processing billion of records by collecting, transforming and pushlishing data to many Data Warehouse and Dashboards though our Data Platform.
Apache Spark and Airflow is the the tools that we choose for batching layer.
In this post I will show you how and why we have a big move from Spark on AWS EMR to Kubernetes.

# Apache Spark

Spark is an open-source, scalable, massively parallel, in-memory execution engine for analytics applications.
From the beginning, we have design to have many jobs running on Apache Spark and Hive on EMR. For a long running of times,
there are many disadvantaged, such as:
- Not easy to scale the node
- Cost
- ...

Since the whole system is a micro-services and event-driven design with many components running on Kubernetes.
We thinking about migrating all the jobs from EMR to Kubernetes.

There are many reasons to run Spark on Kubernetes instead of EMR:
- Cost savings, as we deployed Spark clusters using Amazon EMR, dealing with the complexity of provisioning and bootstrapping at the expense of a per-instance, per-second fee, and EMR operational costs, itâ€™s about $700-$800 for management, not including EC2 instance costs.
- Spark on YARN in EMR also comes with a high maintenance cost.
- Kubernetes provides a practical approach to isolated workloads, limits the use of resources, deploys on-demand, and scales as needed.

# Spark on Kubernetes v1

Since Spark version 2.3.x, Spark supported running clusters managed by Kubernetes. We can submit directly by using `spark-submit` on your command line,
what we need is update the `--master` to `k8s://<api_server_host>:<k8s-apiserver-port>`.
To launch Spark Pi in cluster mode, following this:

```bash
$ bin/spark-submit \
    --master k8s://https://<k8s-apiserver-host>:<k8s-apiserver-port> \
    --deploy-mode cluster \
    --name spark-pi \
    --class org.apache.spark.examples.SparkPi \
    --conf spark.executor.instances=5 \
    --conf spark.kubernetes.container.image=<spark-image> \
    local:///path/to/examples.jar
```

The first architecture of Spark on Kubernetes like the following:

![]()

We using Livy, which is a service enables to interaction with Spark Cluster through RESTful API. Airflow scheduler weill trigger


# Spark on Kubernetes v2
